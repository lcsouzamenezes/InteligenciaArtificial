{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exemplo_Q_Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_MIyKGnM3xE"
      },
      "source": [
        "Precisamos instalar primeiro o gym. Executar o seguinte em um notebook Jupyter deve funcionar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjXylIsIMyt9",
        "outputId": "220e27fd-cbf1-462e-af57-38656487c51b"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOUggRubLaBr"
      },
      "source": [
        "## Q-Learning com Gym\n",
        "Depois de instalado, podemos carregar o ambiente e renderizar sua aparência:\n",
        "Abrindo e mostrando um ambiente pronto do gym que simula o problema do Taxi (versão 3):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOIZcsERLZSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba984837-bdfe-4d4f-90d3-2f137647af18"
      },
      "source": [
        "import gym #importa o gym\n",
        "env = gym.make(\"Taxi-v3\").env #carrega o ambiente\n",
        "env.render() #Renderiza uma estrutura do ambiente (útil na visualização do ambiente)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdwiHPjOQtS"
      },
      "source": [
        "O quadrado preenchido representa o táxi, que é amarelo sem passageiro e verde com passageiro.\n",
        "A barra vertical (\"|\") representa uma parede que o táxi não pode atravessar.\n",
        "R, G, Y, B são os locais de coleta e destino possíveis. A letra azul representa o local atual de embarque do passageiro e a letra roxa é o destino atual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhK_phsuXxox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e366278e-89bb-4145-ce6d-11f06e4717aa"
      },
      "source": [
        "print(\"Total de Ações {}\".format(env.action_space))\n",
        "print(\"Total de Estados {}\".format(env.observation_space))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de Ações Discrete(6)\n",
            "Total de Estados Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNDUC5XNa1AK"
      },
      "source": [
        "### Alterando o estado\n",
        "Podemos codificar o estado do agente e fornecê-lo ao ambiente para renderizar no Gym. Lembre-se de que temos o táxi na linha 3, coluna 1, nosso passageiro está no local 2 e nosso destino é o local 0. Usando o método de codificação de estado Taxi-v3, podemos fazer o seguinte:\n",
        "\n",
        "Use a função env.encode(taxi_linha,taxi_coluna,passageiro_saida,passageiro_chegada)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdAlu52DXyDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166775ef-ea4e-4a84-c7b1-f8fe68472942"
      },
      "source": [
        "state = env.encode(3, 1, 2, 0) \n",
        "print(\"Número do estado:\", state)\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número do estado: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO4_XZWhPSeZ"
      },
      "source": [
        "Outra forma de definir o estado. Podemos definir o estado do ambiente manualmente env.env.s usando esse número codificado.\n",
        "\n",
        "369 correspondente a um estado entre 0 e 499. O ambiente possue 500 estados possíveis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyTvFT55QznG",
        "outputId": "e63f5f22-e287-4d86-cadf-54147dec2309"
      },
      "source": [
        "env.s = 369\n",
        "print(\"Número do estado:\", env.s)\n",
        "env.render()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número do estado: 369\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpV02LMuc5CT"
      },
      "source": [
        "**A Tabela de Recompensas**\n",
        "Quando o ambiente Taxi é criado, também é criada uma tabela inicial de Recompensas, chamada `P`. Podemos pensar nisso como uma matriz que tem o número de estados como linhas e o número de ações como colunas, ou seja, \n",
        " s t a t e s X a c t i o n s\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akeOkzw2Xy1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a3ef00-acc0-4f35-f6db-2b7216e32d32"
      },
      "source": [
        "env.P[329] #estado atual"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 429, -1, False)],\n",
              " 1: [(1.0, 229, -1, False)],\n",
              " 2: [(1.0, 349, -1, False)],\n",
              " 3: [(1.0, 329, -1, False)],\n",
              " 4: [(1.0, 329, -10, False)],\n",
              " 5: [(1.0, 329, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE-PCWT5Q0Ik"
      },
      "source": [
        "Este dicionário tem a estrutura {action: [(probability, nextstate, reward, done)]}.\n",
        "\n",
        "O 0-5 corresponde às ações (sul, norte, leste, oeste, embarque, desembarque) que o táxi pode executar em nosso estado atual na ilustração.\n",
        "Neste env, probabilidade é sempre 1.0.\n",
        "Este next state é o estado em que estaríamos se tomarmos a ação. \n",
        "Todas as ações de movimento têm uma recompensa -1 e as ações pickup (pegar)/dropoff(deixar) têm -10 neste estado particular. Se estivermos em um estado em que o táxi tem um passageiro e está em cima do destino certo, veríamos uma recompensa de 20 na ação.\n",
        "O done é usado para nos dizer quando conseguimos deixar um passageiro no local certo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSFd4dC_eJvq"
      },
      "source": [
        "### Resolvendo usando Ações Aleatórias, sem Aprendizagem por Reforço\n",
        "\n",
        "Vamos ver o que aconteceria se tentarmos usar a força bruta para resolver o problema sem aprendizagem por reforço.\n",
        "\n",
        "Como temos nossa tabela P de recompensas padrão em cada estado, podemos tentar fazer com que nosso táxi navegue usando apenas isso.\n",
        "\n",
        "Vamos criar um loop infinito que corre até que um passageiro chegue a um destino, ou em outras palavras, quando a recompensa recebida for 20.\n",
        "\n",
        "O método env.action_space.sample() seleciona automaticamente uma ação aleatória de um conjunto de todas as ações possíveis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSeXUtwveKEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25aa45dc-aa53-449a-d4aa-439f7202ec34"
      },
      "source": [
        "env.s = 329  # começa no estado do exemplo acima\n",
        "\n",
        "epochs = 0   # total de ações realizadas \n",
        "penalties = 0   # quantidade de punições recebidas por pegar ou largar no lugar errado\n",
        "\n",
        "frames = [] # usado para fazer uma animação\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()  # escolhe aleatoriamente uma ação\n",
        "    state, reward, done, info = env.step(action)  # aplica a ação e pega o resultado\n",
        "\n",
        "    if reward == -10:  # conta uma punição\n",
        "        penalties += 1\n",
        "    \n",
        "    # Guarda a sequência para poder fazer a animação depois\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "    \n",
        "print(\"Total de ações executadas: {}\".format(epochs))\n",
        "print(\"Total de penalizações recebidas: {}\".format(penalties))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de ações executadas: 20457\n",
            "Total de penalizações recebidas: 6649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn3osqjIeKjL"
      },
      "source": [
        "### Mostrando a animação dos movimentos realizados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8uiG5MBgSXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058f5349-e593-4632-c84d-65154945d6d3"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 20457\n",
            "State: 85\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bHbRs7S5o5"
      },
      "source": [
        "Não é bom. Nosso agente leva milhares de passos de tempo e faz muitos desembarques errados para entregar apenas um passageiro ao destino certo.\n",
        "\n",
        "Isso ocorre porque não estamos aprendendo com experiências anteriores. Podemos repetir isso repetidamente e nunca será otimizado. O agente não tem memória de qual ação foi melhor para cada estado, que é exatamente o que o Aprendizado por Reforço fará por nós."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5kNfr9efDvZ"
      },
      "source": [
        "### Aprendendo a resolver usando Q-Learning\n",
        "\n",
        "Essencialmente, o Q-learning permite que o agente use as recompensas do ambiente para aprender, com o tempo, a melhor ação a ser executada em um determinado estado.\n",
        "\n",
        "Em nosso ambiente de táxi, temos a tabela de recompensas P, com a qual o agente aprenderá. Ele procura receber uma recompensa por realizar uma ação no estado atual e, em seguida, atualiza um valor Q para lembrar se essa ação foi benéfica.\n",
        "\n",
        "Os valores armazenados na tabela Q são chamados de valores Q e são mapeados para uma (state, action)combinação.\n",
        "\n",
        "Um valor Q para uma determinada combinação de estado-ação é representativo da \"qualidade\" de uma ação realizada a partir desse estado. Melhores valores de Q implicam em melhores chances de obter maiores recompensas.\n",
        "\n",
        "Por exemplo, se o táxi se depara com um estado que inclui um passageiro em sua localização atual, é altamente provável que o valor Q para pickup seja maior quando comparado a outras ações, como dropoff ou north.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txSfEmTLeKtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac0be8a-9f7d-48c9-d111-07bea754ecc3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Inicialização com a tabela de valores Q\n",
        "# Primeiro, precisamos criar nossa tabela Q, que usaremos para rastrear estados, \n",
        "# ações e recompensas. O número de estados e ações no ambiente Táxi determina o tamanho da nossa mesa.\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hiperparâmetros\n",
        "alpha = 0.1   # taxa de aprendizagem\n",
        "gamma = 0.6   # fator de desconto\n",
        "epsilon = 0.1  # chance de escolha aleatória  \n",
        "\n",
        "# Total geral de ações executadas e penalidades recebidas durante a aprendizagem\n",
        "epochs, penalties = 0,0\n",
        "# treinamento do agente\n",
        "for i in range(1, 100001): # Vai rodar 100000 diferentes versões do problema\n",
        "    state = env.reset()  # Inicialização aleatoria do ambient\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        #épsilon entra em ação quando geramos um valor aleatório entre 0 e 1 e o \n",
        "        #comparamos com nosso épsilon (taxa de exploração), se o valor aleatório for menor,\n",
        "        #executamos uma ação aleatória de nosso espaço de ação\n",
        "        #senão olhamos nossa tabela Q atual e tomamos a ação que maximiza a função de valor.\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Escolhe ação aleatoriamente\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Escolhe ação com base no que já aprendeu\n",
        "\n",
        "        #Depois de escolher uma ação, continuamos com ela e medimos a recompensa associada. \n",
        "        #Isso é feito usando o método embutido env.step (ação) que faz um movimento de um passo de tempo\n",
        "        #Ele retorna o próximo estado, a recompensa da ação anterior; \n",
        "        #done indica se nosso agente atingiu a meta (o que significaria redefinir o ambiente);\n",
        "        # info é apenas um diagnóstico de desempenho usado para depuração.\n",
        "        next_state, reward, done, info = env.step(action) # Aplica a ação\n",
        "        \n",
        "        old_value = q_table[state, action]  # Valor da ação escolhida no estado atual\n",
        "        next_max = np.max(q_table[next_state]) # Melhor valor no próximo estado\n",
        "        \n",
        "        # Atualize o valor Q usando a fórmula principal do Q-Learning\n",
        "        # Finalmente, podemos usar as informações coletadas para atualizar a tabela Q usando a equação de Bellman.\n",
        "        # Lembre-se de que alfa representa a taxa de aprendizado aqui.\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:  # Contabiliza as punições por pegar ou deixar no lugar errado\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state # Muda de estado\n",
        "        epochs += 1\n",
        "        \n",
        "print(\"Treinamento finalizado.\\n\")\n",
        "print(\"Total de ações executadas: {}\".format(epochs))\n",
        "print(\"Total de penalizações recebidas: {}\".format(penalties))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de ações executadas: 1594949\n",
            "Total de penalizações recebidas: 48550\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWMOzsS0fEZb"
      },
      "source": [
        "### Mostrando a tabela Q para o estado 329\n",
        "Agora que a tabela Q foi estabelecida com mais de 100.000 episódios, vamos ver quais são os valores Q no estado de nossa ilustração:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cBAaoqGg1y_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65d4a6b-f0f7-43f0-98da-682fabd978ab"
      },
      "source": [
        "env.s = 329\n",
        "env.render()\n",
        "\n",
        "q_table[329]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.48850115,  -2.47061344,  -2.48794422,  -2.4815022 ,\n",
              "       -10.97219233, -11.02119763])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2L-4fN2g9YO"
      },
      "source": [
        "### Resolvendo o problema com o aprendizado adquirido\n",
        "Vamos avaliar o desempenho do nosso agente. Não precisamos explorar mais as ações, então agora a próxima ação é sempre selecionada usando o melhor valor Q:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mguXdCZ_rwoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6042d7c5-347f-4996-f73c-125cc67a30e7"
      },
      "source": [
        "state = 329\n",
        "epochs, penalties = 0, 0\n",
        "    \n",
        "done = False\n",
        "    \n",
        "while not done:\n",
        "     action = np.argmax(q_table[state])\n",
        "     state, reward, done, info = env.step(action)\n",
        "\n",
        "     if reward == -10:\n",
        "        penalties += 1\n",
        "\n",
        "     epochs += 1\n",
        "\n",
        "print(\"Total de ações executadas: {}\".format(epochs))\n",
        "print(\"Total de penalizações recebidas: {}\".format(penalties))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de ações executadas: 14\n",
            "Total de penalizações recebidas: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT1GFNlJWR6O"
      },
      "source": [
        "Podemos verificar pela avaliação que o desempenho do agente melhorou significativamente e não incorreu em penalidades, o que significa que realizou as ações corretas de embarque / desembarque com 100 passageiros diferentes.\n"
      ]
    }
  ]
}